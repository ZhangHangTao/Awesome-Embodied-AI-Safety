# Awesome-Embodied-AI-Safety [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of publications on safety in Embodied AI, including topics like adversarial attacks, alignment, backdoor, jailbreak, prompt injection, surveys, and safety frameworks.

<strong>Last Update: January 20th, 2025</strong>.

This repository is supported by the Trustworthy Artificial Intelligence (T-AI) Lab at Huazhong University of Science and Technology (HUST). We will try our best to continuously maintain this Github Repository in a weekly manner. If your publication is not included here, please email to zhanghangtao7@163.com


## Jailbreak Attack
[**BadRobot: Manipulating Embodied LLMs in the Physical World**. ArXiv 2024](https://arxiv.org/abs/2407.20242) Citation: 9


## Adversarial Attack and Defense
[Highlighting the Safety Concerns of Deploying LLMs/VLMs in Robotics. ArXiv 2024](https://arxiv.org/abs/2402.10340) Citation: 20

[Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics. ArXiv 2024](https://arxiv.org/abs/2411.13587) Citation: 0

[Rethinking Robustness Assessment: Adversarial Attacks on Learning-based Quadrupedal Locomotion Controllers. ArXiv 2024](https://arxiv.org/abs/2405.12424) Citation: 7

[Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models. ACM MM 2024](https://dl.acm.org/doi/abs/10.1145/3664647.3680616) Citation: 4

[Malicious Path Manipulations via Exploitation of Representation Vulnerabilities of Vision-Language Navigation Systems. IROS 2024](https://ieeexplore.ieee.org/abstract/document/10802618) Citation: 3


## Backdoor Attack and Defense

[**TrojanRobot: Backdoor Attacks Against LLM-based Embodied Robots in the Physical World**. Arxiv 2024](https://arxiv.org/abs/2411.11683) Citation: 2

[Compromising Embodied Agents with Contextual Backdoor Attacks. Arxiv 2024](https://arxiv.org/abs/2408.02882) Citation: 5

[Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems. Arxiv 2024](https://arxiv.org/abs/2405.20774) Citation: 7



## Alignment



## Prompt Injection Attack 



## Safety Frameworks 
