# Awesome-Embodied-AI-Safety [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of publications on safety in Embodied AI, including topics like adversarial attacks, alignment, backdoor, jailbreak, prompt injection, surveys, and safety frameworks.

<strong>Last Update: January 20th, 2025</strong>.

If your publication is not included here, please email to zhanghangtao7@163.com
