# Awesome-Embodied-AI-Safety [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of publications on safety in Embodied AI, including topics like adversarial attacks, alignment, backdoor, jailbreak, prompt injection, surveys, and safety frameworks.

<strong>Last Update: January 20th, 2025</strong>.

This repository is supported by the Trustworthy Artificial Intelligence (T-AI) Lab at Huazhong University of Science and Technology (HUST). We will try our best to continuously maintain this Github Repository in a weekly manner. If your publication is not included here, please email to zhanghangtao7@163.com


## Jailbreak Attack
<a href="https://arxiv.org/abs/2407.20242" style="color: #FF5733; font-weight: bold;">
  BadRobot: Manipulating Embodied LLMs in the Physical World. 2024
</a>

Citation: 9


## Adversarial Attack and Defense


## Backdoor Attack and Defense
[TrojanRobot: Backdoor Attacks Against LLM-based Embodied Robots in the Physical World. 2024](https://arxiv.org/abs/2411.11683)
Citation: 2

## Alignment


## Prompt Injection Attack 


## Safety Frameworks 
